\documentclass{article}
\usepackage{graphicx}                            % Required for inserting images
\usepackage[paper=a4paper, top=1.5cm, bottom=1.5cm, left=2.0cm, right=2.0cm, heightrounded]{geometry}
\usepackage{hyperref}                            % Clickable links (disabled for minimal TeX install)
\usepackage{array}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{adjustbox}
\usepackage{rotating}
\usepackage{float}
\usepackage[justification=centering]{caption}
\usepackage{subcaption} % provides the subfigure environment
\usepackage{listings}
\usepackage{amsmath}
\usepackage{listings}

\usepackage{framed}
\usepackage[nobreak=true]{mdframed}
\usepackage{tikz}
\usepackage{longtable}
\usepackage{pdflscape}
\usepackage{graphicx}
\usepackage{pdfpages}

\usetikzlibrary{positioning, arrows.meta, shapes, calc}

\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt}


\lstset{
  basicstyle=\ttfamily\small,
  frame=single,
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  commentstyle=\color{gray},
  keywordstyle=\color{blue},
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=8pt,
  backgroundcolor=\color{gray!10},
  tabsize=2,
  captionpos=b
}
\graphicspath{ {./img/} }



\begin{document}


\begin{titlepage}
    \begin{center}
        {\includegraphics[width=0.4\textwidth]{telecomparis.png} \par}
        \vspace{1cm}
        {\bfseries\LARGE Télécom Paris \par}
        \vspace{1cm}
        \vspace{2cm}
        {\scshape\Huge Project Robust Key-Value Store \par}
        \vspace{1cm}
            {\itshape\Large Fundamentals of algorithms \par}
        \vfill
        \vspace{1cm}
            {\itshape\Large Professor: KUZNETSOV Petr \par}
        \vspace{2cm}

        {\Large Members: \par}
        {\Large KABIL Aymen \par}
        {\Large FAMÀ Daniele \par}
        {\Large BARAU  Elena\par}
        {\Large MARCULESCU Tudor\par}

        \vfill

        \vfill
        {\Large January 2026 \par}
    \end{center}
\end{titlepage}

\clearpage


\tableofcontents
\clearpage
\listoftables
\listoffigures


\clearpage

\newpage
\section{Introduction}

In this project we are proposing an implementation for the robust key-value store problem in a distributed
system. Our design is based on Akka, which is a middleware that allows to easily create actor based
systems in Java. The ideea is to be able to simulate multiple nodes of the system on a single machine
using multiple actors in the Akka framework. Java 8 jdk can be used to compile and run the project with
Maven as a build tool.

The implementation is split into multiple classes, each one having a specific role in the system.
One of the most important is the Process.java, which contains the behaviour described in the pseudocode
presented in the project description. The main class is responsible for creating the actors, setting
some of them to a crashed state and starting the read and write operations.

Our system is fault tolerant to actors crashing, as long as their number is less then a half. If the
number of crashes exceeds this limit, the system is not able to guarantee the correctness, due to the
posibility of having partitions.

The implementation is tested with multiple scenarios, varying the number of processes in the system,
the number of crashed processes and the number of read and write operations. This is done by the means
of passing arguments to the main class in a Makefile configuration that is provided in the project.
After a successful run, the latency of the operations is printed in the console, as well as the total
time taken for all the operations to complete, which is added in a csv file for further analysis.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 2. System description

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{System description}

The key-value store distributed system is highly concurent, meaning that multiple clients can send
write or read requests at the same time. The non-trivial of concurrency happens when multiple clients
are sending write requests for the same key at the same time, therefore having the same timestamp. This
is explained in the following subsections. In order to understand what the read and write operations
do, the sequential specification of them is explained first.

\subsection{Sequential specification}

The state of a key-value store is a set of key-value pairs The state of a key-value store is a set of key-value pairs of the form (k, v), where k is an integer and v
is a value in a given value set (assume that values are also integers). The initial state is an empty set.
The system exports two operations:

- put(k, v) sets the value with key k to v (overwriting the old value if it is already in the set).

- get(k) returns the value of key k (the default value $\bot$ is returned is the key is not at the system).

\subsection{Actor behaviour description}

To write a new value v, a writer increments its local sequence number r, sends a read request [?, r]
to all the processes, waits for the responses of the type [v', t', r] from a majority (> n/2) of the
processes (we also say a quorum). Then it picks up the highest timestamp tmax in the set of received
responses, computes its new timestamp as t = tmax + 1 and sends a write request [v, t] to all the pro-
cesses in the system. The operation completes as soon as responses of the type [ack, v, t] are received
from a quorum.

To read the register value, a reader increments its sequence number r and sends a read
request [?, r] to every all the processes. As soon as responses of the type [v', t', r] are received
from a quorum, the operation selects the value vm equipped with the highest timestamp tm. If there are
multiple different values written with timestamp tm, the largest such value is chosen. To make sure
that subsequent read operations will not “miss” the returned value, the reader then sends a write request
[vm, tm], waits until a quorum conﬁrms it by sending [ack, vm, tm], and only then returns v.

Notice that, as multiple values can be written with the same timestamp, the value is attached to the
acknowledgement sent to a write request. This is needed to ensure that the acknowledgement is indeed
sent to this write request and not to an earlier write request with the same timestamp but a different value
(this could happen if the operation is concurrent with multiple writes using the same timestamp).
An alternative solution is to maintain an additional sequence number attached to every write request a process issues.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 3. Proof of correcteness

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Proof of correctness}

In order to prove the correcteness of our implementation, we need to show that it satisfies both
safety and liveness properties. Technically, most of the implementation is part of the Process.java
file that contains the actor behaviour in the context of Akka. The main logic is there to handle
actor creation, actor crashing and actor start. This does not affect the correctness of the system,
unless the number of crashed actors exceeds half of the total number of actors, which is not the
case for the testing scenarios that are run in this project.

\subsection{Safety}

The system implemented is similar to an ABD algorithm, but in our case, there are multiple writers
and readers and the implemented registers are atomic. For simplicity, out of the k keys that would
be possible to be used in order to store values, we will focus on the case of a single key. Therefore,
both the writers and readers will be using k = 0 to store and retrieve values. Safety properties allow
us to argue that the system never returns an incorrect value, or trivially, that nothing bad ever
happens.

Since we have multiple writers and readers, one would need to prove that the additional steps that
were added to the ABD algorithm are there to maintain safety and linearizability. When talking about
safety, the behaviour of the system would need to be exactly as the one of the atomic register, for
which the last written value is the one that is retrieved by subsequent read operations. Therefore,
the attention for the safety proof is shifting towards the parts of the implementation where an output
value is returned. In our case, this happens in two places, depending on whether the operation is a
write or a read. Figure \ref{fig:safety-output} shows a code snippet in which the output is returned
as a log message from the Process.java class. The lines addonations are added in order to easily link
the implementation in java to the pseudocode presented in the project description.

\begin{figure}[H]
\begin{lstlisting}
// From Process.java
// lines 13 and 21: wait until received [ack, v, t] from a majority
    if (isWrite) {
        // line 14: return ok
        log.info(processName + ": " + "Put value: " + v + " operation duration: " + timeSpent +"ns");
    } else {
        // line 22: return v_m
        log.info(processName + ": " + "Get return value: " + readenValue + " operation duration: "
        + timeSpent +"ns");
    }
\end{lstlisting}
\caption{Code snippet for output implementation}\label{fig:safety-output}
\end{figure}

If the operation is a write, the process simply returns an "ok" message in the form of what value was
written and the total time taken to finish the operation. Before that, the process needs to know that
what is being written is indeed acknowledged by a quorum of processes. This is guaranteed by the condition
of having received [ack, $v_m$, $t_m$] messages from a majority of processes, as shown in the figure \ref{fig:safety-output}.
Of course, there are some checks that need to be done before counting the received acknowledgements, such as
checking if the process is crashed or not, if it has been launched or not, if the timestamp received
matches the current operation timestamp and if the value received matches the current operation value.


\begin{figure}[H]
\begin{lstlisting}
public void onAck(Ack ack){
        if (isCrashed || !isLaunched) return;
        if (ack.getTimestamp() != currentOpTimestamp) return;
        if (ack.getValue() != currentOpValue) return;
        if (ackSenders.contains(getSender())) return;
        ackSenders.add(getSender());
        if (ackSenders.size() >= (N / 2) + 1) {
            // 11.REQ: Measure latency (End Timer & Calculation)
            long timeSpent = System.nanoTime() - operationStartTime;
            if (isWrite) {
                ...
            }
        ...
        }
}
\end{lstlisting}
\caption{Majority check for write}\label{fig:safety-output}
\end{figure}

Now assuming that a quorum of processes have acknowledged the write operation, we can be sure that at
when a process reads, it is guaranteed to query at least one process that participated in the most
recent successful write. This prevents the reader from returning on old value. However, this is not
sufficient to guarantee that no new-old inversion happens.

The timestamps used for writes need to be monotonically increasing. A process only updates its local
state if the incoming information is "newer" (higher timestamp) or "larger" (tie-breaking by value).
This enforces strictly increasing timestamps on every node. A reader collecting these values will
therefore never see a timestamp decrease over time for the same sequence of operations. This can be
seen in figure \ref{fig:safety-timestamp-update}.

\begin{figure}[H]
\begin{lstlisting}
public void onWriteRequest(WriteRequest message){
    ...
    if((timestampReq > localTimestamp) ||
        (timestampReq == localTimestamp && valueReq > localValue)) {
        localValue = valueReq; // line 25
        localTimestamp = timestampReq; // line 26
    }
    ...
}
\end{lstlisting}
\caption{Timestamp update on write request}\label{fig:safety-timestamp-update}
\end{figure}

To read a value from the system, a process will first query a quorum of processes and wait for their
responses. Once a quorum is reached, the process will select the value with the highest timestamp
and then it will perform a write-back of this value to ensure that subsequent reads will not miss this value.
This can be seen in figure \ref{fig:safety-read-response}. In order to ensure that the responders
are unique, a set "readResponseSenders" is used to keep track of them.

\begin{figure}[H]
\begin{lstlisting}

public void onReadResponse(ProcessMessage message) {
    if(isCrashed || !isLaunched) return;
    if(message.getSequenceNumber() != sequenceNumber) return;
    if(readResponseSenders.contains(getSender())) return;
    readResponseSenders.add(getSender());
    // lines 9 and 18 (collect responses)
    readResponses.add(message);
    // majority reached lines 9 and 18
    if(readResponseSenders.size() >= (N / 2) + 1){
        int maxTs = Integer.MIN_VALUE;
        int maxVal = Integer.MIN_VALUE;
        for(ProcessMessage m: readResponses){
            if(m.getTimestamp() > maxTs){
                maxTs = m.getTimestamp(); // lines 10 and 19
                maxVal = m.getValue(); // line 19
            } else if (m.getTimestamp() == maxTs && m.getValue() > maxVal) {
                maxVal = m.getValue(); // line 19
            }
        }
        // lines 20-21: write-back
        // handle onAck to return the value
        }
}
\end{lstlisting}
\caption{Read response handling}\label{fig:safety-read-response}
\end{figure}

Therefore, the impossibility of reading an old value after a new one has been successfully written is guaranteed
by the intersection of the quorums (the one used in the write operation and the one used in the read operation),
as well as the monotonically increasing timestamps.
Fundamentally, the implementation defines a successful write operation as one that has been acknowledged
by a strict majority of actors ($N/2 + 1$), ensuring that the new value and its associated higher timestamp
are physically stored on more than half of the actors in the system. When a subsequent read operation
is initiated, it is similarly required to query and receive responses from a majority of actors before
proceeding. By the laws of set theory, the set of actors that stored the write and the set of nodes responding
to the read must overlap by at least one actor. This overlapping actor acts holds
the updated state. Since the code in onWriteRequest enforces that nodes only update their local storage
if the incoming timestamp is strictly higher (or the value is larger for the same timestamp), this actor
is guaranteed to possess and report the most recent timestamp. On the reader’s logic in onReadResponse
there are aggregated all incoming messages and deterministically selects the value associated with the highest
timestamp found (maxTs). Because the intersection actor is guaranteed to report the new, higher timestamp, the
reader’s comparison logic will inevitably select this new value over any previous value reported by nodes
that missed the most recent write.

All in all linearizability is preserved and the safety property is satisfied.

\subsection{Linearizability validation}

We validate linearizability by extracting an execution history from the actor logs and deciding whether there exists a total order consistent with real-time and the atomic single-register specification. Each operation is characterized by its type (put/get), value and the interval between invocation and completion. We define a precedence relation with an edge \(o_i \to o_j\) whenever \(\operatorname{end}(o_i) \le \operatorname{start}(o_j)\); any linearization must extend this partial order. The decision procedure enforces the specification: puts update the register at their position in the total order and gets return the latest value preceding them.

For small histories, we use an exact search that extends the precedence relation to a total order satisfying the specification. For larger histories, we apply a scalable check for reads: for a read \(r\) returning value \(v\), (i) some write of \(v\) must have completed no later than \(\operatorname{end}(r)\); and (ii) no write of a strictly larger value may have completed before \(\operatorname{start}(r)\). This criterion is consistent with ABD-style timestamp monotonicity and value tie-breaking.

Instrumentation records one invocation and one completion event per operation at the points where sequence numbers advance and where quorums are reached. Logging is enabled without changing behavior via \verb|KV_LOG| or \verb|-Dkv.log|.

For the case when having \(N=3, f=1, M=3\), a typical trace includes a completed put of 4, a completed read returning 4 and a completed put of 7. A valid linearization is \([\mathrm{Put}(4)] < [\mathrm{Get}(4)] < [\mathrm{Put}(7)]\): the read is placed after the first put and before the second, so it returns 4, which matches the log. In runs where two puts overlap, the read is placed according to real-time and returns the value written by the put with the maximal (timestamp, value) among the completed writes preceding it. Across our baseline executions with \(N=3, f=1, M=3\), all histories satisfied these conditions, so the instance is linearizable.

For larger randomized scenarios, the validator either certifies linearizability or identifies a read whose returned value cannot be justified by the completed writes under the real-time constraints, aiding inspection of the run and the logging alignment.

\subsection{Liveness}

To argue about liveness, we need to set the fact that the channel is reliable: the messages are
eventually delivered without being duplicated.

The implemented algorithm relies on the fundamental assumption that a majority of processes ($N/2 + 1$)
remain reachable and correct. Despite a certain number of actors being set in a crashed state by main,
the system continues always with a majority of correct and reachable processes in our testing scenarios.

Each process in the system broadcasts it's ReadRequest and WriteRequest to the entire system of $N$ nodes,
therefore it does not need to wait for any specific peer to respond; it only requires responses from
any available majority. The broadcast of the message is done in the figure \ref{fig:liveness-broadcast}.
The actorRefList is a list of all the ActorRef instances representing the other processes in the system.
It is updated during the initialization phase when each actor is created.

\begin{figure}[H]
\begin{lstlisting}
private void broadcastMessage(Object msg){
    for (ActorRef actor : actorRefList) {
        actor.tell(msg, self());
    }
}
\end{lstlisting}
\caption{Broadcasting read request}\label{fig:liveness-broadcast}
\end{figure}

The response collection mechanism can be reffered to in the figures \ref{fig:safety-output} and \ref{fig:safety-read-response}.
where, depending on whether the expected response consists of [v', t', r] or [ack, v, t]. One can see
that N/2 + 1 is a threshold that after is reached, the process can proceed to the next step, without needing to wait
for any specific response. Once this threshold is met, the algorithm performs a finite number of computational
steps to calculate the maximum timestamp or verify the write, guaranteeing that the operation terminates
and the process proceeds to the next sequence number.

The onReadRequest and onWriteRequest handlers are designed to be non-blocking and immediate.
They can be refered in the following figure

\begin{figure}[H]
\begin{lstlisting}
public void onWriteRequest(WriteRequest message){
    if(isCrashed) return;
    int timestampReq = message.getTimestamp();
    int valueReq = message.getValue();
    // line 24: if t' > localTS or (t' = localTS and v' > localValue)
    if((timestampReq > localTimestamp) ||
    (timestampReq == localTimestamp && valueReq > localValue))
    {
        localValue = valueReq; // line 25
        localTimestamp = timestampReq; // line 26
    }
    // line 27: send [ack, v', t'] to p
    getSender().tell(new Ack(valueReq, timestampReq), self());
}

public void onReadRequest(ReadRequest message){
    if(isCrashed) return;
    // line 29: send [localValue, localTimestamp, r'] to p_j
    getSender().tell(new ProcessMessage(localValue,
            localTimestamp, message.getSequenceNumber()), self());
}
\end{lstlisting}
\caption{Read and write request handlers}\label{fig:liveness-request-handlers}
\end{figure}


There are no circular dependencies or deadlocks between actors. Consequently, as long as the network
eventually delivers messages and the number of crashed nodes does not exceed the failure threshold ($f < N/2$),
the accumulating responses in the initiator's readResponses or ackSenders collections will inevitably
cross the quorum threshold.

\subsection{Conclusion on correctness}

Correctness is demonstrated through two complementary properties: Safety and Liveness. Safety is preserved
by the mathematical guarantee of quorum intersections combined with monotonic timestamps, ensuring that
once a write is acknowledged by a majority, no subsequent read can return a stale value (Atomic Consistency).
Liveness is maintained by the asynchronous, non-blocking nature of the actor model, which guarantees
that operations will eventually complete provided that a majority of processes remain operational
(tolerance of $f < N/2$ failures). Together, these mechanisms ensure that the distributed processes
behave indistinguishably from a single, centralized shared memory, satisfying the sequential specification
despite the underlying network asynchrony and node failures.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 4. Performance evaluation

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section {Performance evaluation}

Performance evaluation of our implementation is based on measuring the time taken for all the read
and write operations to complete. There are multiple factors that can influence this time, such as the total number of processes in the system and the number of write and read operations.

\subsection{Experimental setup}

Our test suite is implemented such that the number of processes in the system N, the number of failed processes f and the number of read and write operations M can be configured before starting the
program. N is varied between 3, 10 and 100, while f is set to be equal to 1, 4 or 49. M is also varied between 3, 10 and 100.
Table \ref{tab:performance-obtained} presents the complete set of tested configurations and the corresponding latency results.

\subsection{Discussion}

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Number of processes N} & \textbf{Number of failed processes f} & \textbf{Number of operations } & \textbf{Latency [ms]} \\
\hline

3    & 1                 & 3           & 10        \\
3    & 1                 & 10          & 23        \\
3    & 1                 & 100         & 113       \\
10   & 4                 & 3           & 61        \\
10   & 4                 & 10          & 79        \\
10   & 4                 & 100         & 201       \\
100  & 49                & 3           & 231       \\
100  & 49                & 10          & 353       \\
100  & 49                & 100         & 1343      \\

\hline
\end{tabular}
\caption{Performance obtained}\label{tab:performance-obtained}
\end{table}

\begin{figure}
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/lat_vs_n.pdf}
        \caption{Total latency as a function of number of processes ($N$)}
        \label{fig:lat_vs_n}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/lat_vs_m.pdf}
        \caption{Total latency as a function of number of operations per process ($M$)}
        \label{fig:lat_vs_m}
    \end{subfigure}
    \caption{Latency as a function of number of processes and operations}
    \label{fig:lat_vs_n_m}
\end{figure}

\textbf{Impact of number of processes}($N$):
The number of nodes has a significant impact on performance. 
As $N$ increases, the quorum size (majority required for read/write) increases proportionally. For $N=100$, a process must successfully exchange messages with at least 51 processes (versus 2 processes for $N=3$). 
This leads to higher network traffic and forces the process to wait for a larger set of responses (increasing the probability of waiting for slower nodes).
Comparing the results for a high workload ($M=100$):
\begin{itemize}
    \item $N=3$: 113 ms
    \item $N=100$: 1343 ms
\end{itemize}
The latency increases by an order of magnitude, reflecting the cost of communicating with a much larger quorum. 
It is worth noting that the performance penalty of increasing the system size from $N=3$ to $N=10$ is more pronounced for short workloads ($M=3, 10$) than for long workloads ($M=100$).
Comparing the transition from $N=3$ to $N=10$, we observe that the latency growth factor decreases as the workload $M$ increases.
Specifically, for a moderate workload ($M=10$), the latency grows by a factor of $\approx 3.4\times$ (from 23ms to 79ms). However, for a heavier workload ($M=100$), the growth factor drops to $\approx 1.8\times$ (from 113ms to 201ms).
This indicates that the overhead of increasing the system size is heavily front-loaded in the initialization phase (actor creation and network setup), which scales poorly with $N$.
In contrast, the operational phase (quorum communication) scales efficiently due to asynchronous message passing.
Consequently, as $M$ increases, the total execution time becomes increasingly dominated by the efficient operational phase, thereby strictly reducing the relative performance penalty of scaling up the system size.

\textbf{Impact of Workload} ($M$):
As expected, the latency increases with the number of operations $M$ (Figure \ref{fig:lat_vs_m}). 
For example, in the $N=3$ configuration, increasing $M$ from 10 to 100 results 
in a latency increase from 23ms to 113ms. This linear trend confirms that the individual operation processing time remains relatively stable even as the sequence of operations grows.

\subsection{Conclusion}

The implementation successfully scales to $N=100$ while maintaining correctness 
in the presence of $f < N/2$ failures. While latency inevitably grows with $N$ 
due to the mechanics of the ABD-like majority consensus9, the system remains responsive. 
The experiments confirm that the protocol correctly handles the trade-off between fault 
tolerance and performance latency.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 5. Conclusion

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Conclusion}

% The implementation successfully scales to $N=100$ while maintaining correctness 
% in the presence of $f < N/2$ failures. While latency inevitably grows with $N$ 
% due to the mechanics of the ABD-like majority consensus, the system remains responsive. 
% The experiments confirm that the protocol correctly handles the trade-off between fault 
% tolerance and performance latency.

\newpage
\section{Conclusion}

To conclude our report, we can say that our implementation of a robust key-value store in a distributed
system is able to satisfy both safety and liveness properties, making it a reliable solution for
storing and retrieving data in a distributed environment. The use of Akka as a middleware allowed us
to easily create an actor-based system that can handle multiple clients concurrently, while also
being fault-tolerant to actor crashes. Overall, our project demonstrates the feasibility of building
a robust key-value store in a distributed system.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SECTION BIBLIOGRAPHY

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage % keep bibliography on a new page
\bibliographystyle{plain}   % choose a style
\bibliography{ref}      % name of .bib file (no .bib extension)


\end{document}