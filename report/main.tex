\documentclass{article}
\usepackage{graphicx}                            % Required for inserting images
\usepackage[paper=a4paper, top=1.5cm, bottom=1.5cm, left=2.0cm, right=2.0cm, heightrounded]{geometry}
\usepackage{hyperref}                            % Clickable links (disabled for minimal TeX install)
\usepackage{array}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{adjustbox}
\usepackage{rotating}
\usepackage{float}
\usepackage[justification=centering]{caption}
\usepackage{subcaption} % provides the subfigure environment
\usepackage{amsmath}
\usepackage{listings}

\usepackage{framed}
\usepackage[nobreak=true]{mdframed}
\usepackage{tikz}
\usepackage{longtable}
\usepackage{pdflscape}
\usepackage{graphicx}
\usepackage{pdfpages}

\usetikzlibrary{positioning, arrows.meta, shapes, calc}

\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt}


\lstset{
  basicstyle=\ttfamily\small,
  frame=single,
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  commentstyle=\color{gray},
  keywordstyle=\color{blue},
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=8pt,
  backgroundcolor=\color{gray!10},
  tabsize=2,
  captionpos=b
}
\graphicspath{ {./img/} }



\begin{document}


\begin{titlepage}
    \begin{center}
        {\includegraphics[width=0.4\textwidth]{telecomparis.png} \par}
        \vspace{1cm}
        {\bfseries\LARGE Télécom Paris \par}
        \vspace{1cm}
        \vspace{2cm}
        {\scshape\Huge Project Robust Key-Value Store \par}
        \vspace{1cm}
            {\itshape\Large Fundamentals of algorithms \par}
        \vfill
        \vspace{1cm}
            {\itshape\Large Professor: KUZNETSOV Petr \par}
        \vspace{2cm}

        {\Large Members: \par}
        {\Large KABIL Aymen \par}
        {\Large FAMÀ Daniele \par}
        {\Large BARAU  Elena\par}
        {\Large MARCULESCU Tudor\par}

        \vfill

        \vfill
        {\Large January 2026 \par}
    \end{center}
\end{titlepage}

\clearpage


\tableofcontents
\clearpage
\listoftables
\listoffigures


\clearpage

\newpage
\section{Introduction}

In this project we are proposing an implementation for the robust key-value store problem in a distributed
system. Our design is based on Akka, which is a middleware that allows to easily create actor based
systems in Java. The idea is to be able to simulate multiple nodes of the system on a single machine
using multiple actors in the Akka framework. Java 8 jdk can be used to compile and run the project with
Maven as a build tool.

The implementation is split into multiple classes, each one having a specific role in the system.
A core component of the implementation is the \texttt{Process.java} class, which encapsulates the behaviour described in the provided pseudocode \cite{abd}. The main class is responsible for bootstrapping the system: creating the actors, simulating crash failures by setting specific actors to a silent state, and triggering the read and write operation sequences.

Our system provides fault tolerance against process crashes, provided that the number of faulty processes $f$ satisfies $f < N/2$. If the number of crashes exceeds this threshold, the liveness and safety properties cannot be guaranteed, as a quorum (majority) may no longer be obtainable.

The implementation is tested with multiple scenarios, varying the number of processes in the system,
the number of crashed processes and the number of read and write operations. This is done by the means
of passing arguments to the main class in a Makefile configuration that is provided in the project.
After a successful run, the latency of the operations is printed in the console, as well as the total
time taken for all the operations to complete, which is added in a csv file for further analysis.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 2. System description

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{System description}

The implemented key-value store system is highly concurrent, meaning that multiple clients can send
write or read requests at the same time. The non-trivial aspect of concurrency happens when multiple
writes happen at the same time as the reads. The system needs to be able to handle these situations
correctly, ensuring that the read operations always return the most recent written value.

In the following subsection the behaviour of the system is described. First the sequential specification
is presented, as the high level behaviour that the implementation needs to satisfy. That means that
one can interact with the processes as if they were a single shared memory, through put() and get()
operations. Then, the actor behaviour description is presented, which is the actual implementation
of the system, based on the pseudocode provided in the project description.

\subsection{Sequential specification}
The state of a key-value store is a set of key-value pairs of the form (k, v), where k is an integer and v
is a value in a given value set (assume that values are also integers). The initial state is an empty set.
The system exports two operations:

- put(k, v) sets the value with key k to v (overwriting the old value if it is already in the set).

- get(k) returns the value of key k (the default value $\bot$ is returned if the key is not in the system).

A constraint to be taken into consideration is that \texttt{put} and \texttt{get} operations will always be invoked on a fixed key $k=1$, effectively reducing the problem to implementing a single multi-writer multi-reader atomic register, as suggested in the project guidelines.

\subsection{Actor behaviour description}

To write a new value v, a writer increments its local sequence number r, sends a read request [?, r]
to all the processes, waits for the responses of the type [v', t', r] from a majority ($> n/2$) of the
processes (we also say a quorum). Then it picks up the highest timestamp tmax in the set of received
responses, computes its new timestamp as t = tmax + 1 and sends a write request [v, t] to all the processes
in the system. The operation completes as soon as responses of the type [ack, v, t] are received
from a quorum.

To read the register value, a reader increments its sequence number $r$ and broadcasts a read request $[?, r]$ to all processes. As soon as responses of the type $[v', t', r]$ are received from a quorum, the operation selects the value $v_m$ associated with the highest timestamp $t_m$. If there are multiple distinct values with timestamp $t_m$, the largest value is chosen to ensure determinism. To ensure linearizability (specifically, that subsequent reads do not return an older value), the reader performs a \textit{write-back} phase: it sends a write request $[v_m, t_m]$, waits until a quorum confirms it by sending $[ack, v_m, t_m]$, and only then returns $v_m$.

Notice that, as multiple values can be written with the same timestamp, the value is attached to the
acknowledgement sent to a write request. This is needed to ensure that the acknowledgement is indeed
sent to this write request and not to an earlier write request with the same timestamp but a different value
(this could happen if the operation is concurrent with multiple writes using the same timestamp).

\subsection{Implementation details}

All actors in the system are instances running the Process.java class. Each actor has a unique id, a local
timestamp and a local value. There is no shared memory between the actors, the system relies on message
passing for inter actor communication. However, as a whole, the system behaves as a single shared memory.

Each actor can be in a crashed or launched state. If it is crashed, it will not respond to any any
messages. This is done by saving a boolean isCrashed variable in the actor state, which is checked
at the beginning of each message handler.

The behaviour of the actors is defined by event handlers, that being, the actions taken are event
driven. When a message is received, the corresponding handler is executed. It is not a good idea
to block the actor while waiting the responses, so, instead, the responses are collected in a list
and when the number of responses reaches a quorum, the actor can proceed to the next step.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 3. Proof of correctness

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Proof of correctness}

In order to prove the correctness of our implementation, we need to show that it satisfies both
safety and liveness properties. Technically, most of the implementation is part of the Process.java
file, which contains the actor behaviour. The main logic is there to handle
actor creation, actor crashing and actor start. This does not affect the correctness of the system,
unless the number of crashed actors exceeds half of the total number of actors, which is not the
case for the testing scenarios that are run in this project. After the actors are launched, the main
class does not interfere with their behaviour.

\subsection{Safety}

The system implemented is similar to an ABD algorithm, but in our case, there are multiple writers
and readers and the implemented register is atomic. Safety properties allow
us to argue that the system never returns an incorrect value, or trivially, that nothing bad ever
happens.

Since we have multiple writers and readers, one would need to prove that the additional steps that
were added to the ABD algorithm, are there to maintain safety and linearizability. If the behaviour
of the system is proved to be exactly as the one of the atomic register, for which linearizability is guaranteed,
then the safety property is satisfied. Therefore,
the attention for the safety proof is shifting towards the parts of the implementation where an output
value is returned. In our case, this happens in two places, depending on whether the operation is a
write or a read. Figure \ref{fig:safety-output} shows a code snippet in which the output is returned
as a log message from the Process.java class. The lines comments are added in order to easily link
the implementation in java to the pseudocode presented in the project description.

\begin{figure}[H]
\begin{lstlisting}
// From Process.java
// lines 13 and 21: wait until received [ack, v, t] from a majority
    if (isWrite) {
        // line 14: return ok
        log.info(processName + ": " + "Put value: " + v + " operation duration: " + timeSpent +"ns");
    } else {
        // line 22: return v_m
        log.info(processName + ": " + "Get return value: " + readenValue + " operation duration: "
        + timeSpent +"ns");
    }
\end{lstlisting}
\caption{Code snippet for output implementation}\label{fig:safety-output}
\end{figure}

If the operation is a write, the process simply returns an "ok" message in the form of what value was
written and the total time taken to finish the operation. Before that, the process needs to know that
what is being written is indeed acknowledged by a quorum of processes. This is guaranteed by the condition
of having received [ack, $v_m$, $t_m$] messages from a majority of processes, as shown in the figure \ref{fig:safety-output}.
Of course, there are some checks that need to be done before counting the received acknowledgements, such as
checking if the process is crashed or not, if it has been launched or not, if the timestamp received
matches the current operation timestamp and if the value received matches the current operation value.

\begin{figure}[H]
\begin{lstlisting}
public void onAck(Ack ack){
        if (isCrashed || !isLaunched) return;
        if (ack.getTimestamp() != currentOpTimestamp) return;
        if (ack.getValue() != currentOpValue) return;
        if (ackSenders.contains(getSender())) return;
        ackSenders.add(getSender());
        if (ackSenders.size() >= (N / 2) + 1) {
            // 11.REQ: Measure latency (End Timer & Calculation)
            long timeSpent = System.nanoTime() - operationStartTime;
            if (isWrite) {
                ...
            }
        ...
        }
}
\end{lstlisting}
\caption{Majority check for write}\label{fig:safety-output}
\end{figure}

Assuming that a quorum of processes have acknowledged a write operation, any subsequent read operation is guaranteed to query at least one process that participated in that most recent successful write. This is due to the property of \textit{Quorum Intersection}: any two majority sets must have a non-empty intersection ($W \cap R \neq \emptyset$). Consequently, the reader will observe the most recent timestamp/value pair. However, to strictly satisfy atomicity and prevent new-old inversion (where a later read returns an older value than an earlier read), the write-back phase described in the pseudocode is essential.

The timestamps used for writes need to be monotonically increasing. A process only updates its local
state if the incoming information is "newer" (higher timestamp) or "larger" (tie-breaking by value).
This enforces strictly increasing timestamps on every node, therefore a total order of writes,
as seen in figure \ref{fig:safety-timestamp-update}.

\begin{figure}[H]
\begin{lstlisting}
public void onWriteRequest(WriteRequest message){
    ...
    if((timestampReq > localTimestamp) ||
        (timestampReq == localTimestamp && valueReq > localValue)) {
        localValue = valueReq; // line 25
        localTimestamp = timestampReq; // line 26
    }
    ...
}
\end{lstlisting}
\caption{Timestamp update on write request}\label{fig:safety-timestamp-update}
\end{figure}

To read a value from the system, a process will first query a quorum of processes and wait for their
responses. Once a quorum is reached, the process will select the value with the highest timestamp
and then it will perform a write-back of this value to ensure that subsequent reads will not miss this value.
This can be seen in figure \ref{fig:safety-read-response}. In order to ensure that the responders
are unique, a set "readResponseSenders" is used to keep track of them.

\begin{figure}[H]
\begin{lstlisting}

public void onReadResponse(ProcessMessage message) {
    if(isCrashed || !isLaunched) return;
    if(message.getSequenceNumber() != sequenceNumber) return;
    if(readResponseSenders.contains(getSender())) return;
    readResponseSenders.add(getSender());
    // lines 9 and 18 (collect responses)
    readResponses.add(message);
    // majority reached lines 9 and 18
    if(readResponseSenders.size() >= (N / 2) + 1){
        int maxTs = Integer.MIN_VALUE;
        int maxVal = Integer.MIN_VALUE;
        for(ProcessMessage m: readResponses){
            if(m.getTimestamp() > maxTs){
                maxTs = m.getTimestamp(); // lines 10 and 19
                maxVal = m.getValue(); // line 19
            } else if (m.getTimestamp() == maxTs && m.getValue() > maxVal) {
                maxVal = m.getValue(); // line 19
            }
        }
        // lines 20-21: write-back
        // handle onAck to return the value
        }
}
\end{lstlisting}
\caption{Read response handling}\label{fig:safety-read-response}
\end{figure}

Notice that every read operation can be ordered in the history of the system, right after its corresponding
write operation. This is because its output is timestamped, so the history will reflect the one
of an atomic register.

The only way to violate the sequential specification would be to reverse the order of a write and
read operation. Assuming that a read operation precedes a write operation, its timestamp must
be less than the write operation timestamp, therefore the read cannot return the value written
by the write operation. If the write operation precedes the read operation, then the read operation
must query at least one process that participated in the write operation, therefore it cannot
return an old value.

So, in this way, one can assume that that the history of operations in the system can be linearized,
therefore satisfying safety.

\subsection{Linearizability validation}

We validate linearizability by extracting an execution history from the actor logs and deciding whether there exists a total order consistent with real-time and the atomic single-register specification. Each operation is characterized by its type (put/get), value and the interval between invocation and completion. We define a precedence relation with an edge \(o_i \to o_j\) whenever \(\operatorname{end}(o_i) \le \operatorname{start}(o_j)\); any linearization must extend this partial order. The decision procedure enforces the specification: puts update the register at their position in the total order and gets return the latest value preceding them.

For small histories, we use an exact search that extends the precedence relation to a total order satisfying the specification. For larger histories, we apply a scalable check for reads: for a read \(r\) returning value \(v\), (i) some write of \(v\) must have completed no later than \(\operatorname{end}(r)\); and (ii) no write of a strictly larger value may have completed before \(\operatorname{start}(r)\). This criterion is consistent with ABD-style timestamp monotonicity and value tie-breaking.

Instrumentation records one invocation and one completion event per operation at the points where sequence numbers advance and where quorums are reached. Logging is enabled without changing behaviour via \verb|KV_LOG| or \verb|-Dkv.log|.

For the case when having \(N=3, f=1, M=3\), a typical trace includes a completed put of 4, a completed read returning 4 and a completed put of 7. A valid linearization is \([\mathrm{Put}(4)] < [\mathrm{Get}(4)] < [\mathrm{Put}(7)]\): the read is placed after the first put and before the second, so it returns 4, which matches the log. In runs where two puts overlap, the read is placed according to real-time and returns the value written by the put with the maximal (timestamp, value) among the completed writes preceding it. Across our baseline executions with \(N=3, f=1, M=3\), all histories satisfied these conditions, so the instance is linearizable.

For larger randomized scenarios, the validator either certifies linearizability or identifies a read whose returned value cannot be justified by the completed writes under the real-time constraints, aiding inspection of the run and the logging alignment.

\subsection{Liveness}

To argue about liveness, we need to set the fact that the channel is reliable: the messages are
eventually delivered without being duplicated.

The implemented algorithm relies on the fundamental assumption that a majority of processes ($N/2 + 1$)
remain reachable and correct. Despite a certain number of actors being set in a crashed state by main,
the system continues always with a majority of correct and reachable processes in our testing scenarios.

Each process in the system broadcasts its ReadRequest and WriteRequest to the entire system of $N$ nodes,
therefore it does not need to wait for any specific peer to respond; it only requires responses from
any available majority. Eventually, a response will come from a majority, so each read and write operation
will eventually complete.

\begin{figure}[H]
\begin{lstlisting}
private void broadcastMessage(Object msg){
    for (ActorRef actor : actorRefList) {
        actor.tell(msg, self());
    }
}
\end{lstlisting}
\caption{Broadcasting read request}\label{fig:liveness-broadcast}
\end{figure}

The response collection mechanism is illustrated in Figures \ref{fig:safety-output} and \ref{fig:safety-read-response}.
where, depending on whether the expected response consists of [v', t', r] or [ack, v, t]. One can see
that N/2 + 1 is a threshold that after is reached, the process can proceed to the next step, without needing to wait
for any specific response. Once this threshold is met, the algorithm performs a finite number of computational
steps to calculate the maximum timestamp or verify the write, guaranteeing that the operation terminates
and the process proceeds to the next sequence number.

The onReadRequest and onWriteRequest handlers are designed to be non-blocking and immediate.
They can be referred to in the following figure

\begin{figure}[H]
\begin{lstlisting}
public void onWriteRequest(WriteRequest message){
    if(isCrashed) return;
    int timestampReq = message.getTimestamp();
    int valueReq = message.getValue();
    // line 24: if t' > localTS or (t' = localTS and v' > localValue)
    if((timestampReq > localTimestamp) ||
    (timestampReq == localTimestamp && valueReq > localValue))
    {
        localValue = valueReq; // line 25
        localTimestamp = timestampReq; // line 26
    }
    // line 27: send [ack, v', t'] to p
    getSender().tell(new Ack(valueReq, timestampReq), self());
}

public void onReadRequest(ReadRequest message){
    if(isCrashed) return;
    // line 29: send [localValue, localTimestamp, r'] to p_j
    getSender().tell(new ProcessMessage(localValue,
            localTimestamp, message.getSequenceNumber()), self());
}
\end{lstlisting}
\caption{Read and write request handlers}\label{fig:liveness-request-handlers}
\end{figure}

There are no circular dependencies or deadlocks between actors. Consequently, as long as the network
eventually delivers messages and the number of crashed nodes does not exceed the failure threshold ($f < N/2$),
the accumulating responses in the initiator's readResponses or ackSenders collections will inevitably
cross the quorum threshold.

Therefore liveness is guaranteed by the reliable channels and the majority assumption.

\subsection{Conclusion on correctness}

Correctness is demonstrated through two complementary properties: Safety and Liveness. Safety is preserved
by the mathematical guarantee of quorum intersections combined with monotonic timestamps, ensuring that
once a write is acknowledged by a majority, no subsequent read can return a stale value.
Liveness is maintained by the asynchronous, non-blocking nature of the actor model, which guarantees
that operations will eventually complete provided that a majority of processes remain operational.
Together, these mechanisms ensure that the distributed processes
behave indistinguishably from a single, centralized shared memory, satisfying the sequential specification
despite the underlying network asynchrony and node failures.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 4. Performance evaluation

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section {Performance evaluation}

Performance evaluation of our implementation is based on measuring the time taken for all the read
and write operations to complete. There are multiple factors that can influence this time, such as the total number of processes in the system and the number of write and read operations.

\subsection{Experimental setup}

Our test suite is implemented such that the number of processes in the system N, the number of failed processes f and the number of read and write operations M can be configured before starting the
program. N is varied between 3, 10 and 100, while f is set to be equal to 1, 4 or 49. M is also varied between 3, 10 and 100.
Table \ref{tab:performance-obtained} presents the complete set of tested configurations and the corresponding latency results.

The command used to run the battery of tests is:
\begin{mdframed}
    \centering
    make all
\end{mdframed}

This will run all the configurations and append the results in a csv file named "benchmark\_results.csv".
The latency for each operation is measured in nanoseconds, but the total time taken for the program
is presented in milliseconds for better readability.

\subsection{Discussion}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Number of processes N} & \textbf{Number of failed processes f} & \textbf{Number of operations } & \textbf{Latency [ms]} \\
\hline

3    & 1                 & 3           & 10        \\
3    & 1                 & 10          & 23        \\
3    & 1                 & 100         & 113       \\
10   & 4                 & 3           & 61        \\
10   & 4                 & 10          & 79        \\
10   & 4                 & 100         & 201       \\
100  & 49                & 3           & 231       \\
100  & 49                & 10          & 353       \\
100  & 49                & 100         & 1343      \\

\hline
\end{tabular}
\caption{Performance obtained}\label{tab:performance-obtained}
\end{table}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/lat_vs_n.pdf}
        \caption{Total latency as a function of number of processes ($N$)}
        \label{fig:lat_vs_n}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/lat_vs_m.pdf}
        \caption{Total latency as a function of number of operations per process ($M$)}
        \label{fig:lat_vs_m}
    \end{subfigure}
    \caption{Latency as a function of number of processes and operations}
    \label{fig:lat_vs_n_m}
\end{figure}

The number of actors has a significant impact on performance.
As $N$ increases, the quorum size (majority required for read/write) increases proportionally. For $N=100$, a process must successfully exchange messages with at least 51 processes (versus 2 processes for $N=3$).
This leads to messaging traffic and forces the process to wait for a larger set of responses (increasing the probability of waiting for slower nodes).
Comparing the results for a high workload ($M=100$):
\begin{itemize}
    \item $N=3$: 113 ms
    \item $N=100$: 1343 ms
\end{itemize}
The latency increases by an order of magnitude, reflecting the cost of communicating with a much larger quorum.
It is worth noting that the performance penalty of increasing the system size from $N=3$ to $N=10$ is more pronounced for short workloads ($M=3, 10$) than for long workloads ($M=100$). As $M$ increases, the initial overhead of actor creation and network setup is amortized over a larger number of operations. Consequently, the total execution time becomes increasingly dominated by the efficient asynchronous operational phase.
Specifically, for a moderate workload ($M=10$), the latency grows by a factor of $\approx 3.4\times$ (from 23ms to 79ms). However, for a heavier workload ($M=100$), the growth factor drops to $\approx 1.8\times$ (from 113ms to 201ms).
This indicates that the overhead of increasing the system size is heavily front-loaded in the initialization phase (actor creation and network setup), which scales poorly with $N$.
In contrast, the operational phase (quorum communication) scales efficiently due to asynchronous message passing.
Consequently, as $M$ increases, the total execution time becomes increasingly dominated by the efficient operational phase, thereby strictly reducing the relative performance penalty of scaling up the system size.

Also, a point of interest is the logging mechanism that is used to return the invoke and complete
events for each operation. This is done in order to be able to validate linearizability of the system.
However, logging introduces overhead in terms of tens of milliseconds per total run when
the number of operations is low (M=3 or M=10).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 5. Conclusion

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Conclusion}

% The implementation successfully scales to $N=100$ while maintaining correctness
% in the presence of $f < N/2$ failures. While latency inevitably grows with $N$
% due to the mechanics of the ABD-like majority consensus, the system remains responsive.
% The experiments confirm that the protocol correctly handles the trade-off between fault
% tolerance and performance latency.

\section{Conclusion}

In this project, we successfully implemented a robust, distributed key-value store using the Akka framework. By adhering to the ABD algorithm principles, our system guarantees linearizability (safety) and eventual termination (liveness) in the presence of crash failures, provided that a majority of processes remain correct ($f < N/2$).

Our performance evaluation demonstrates that while the latency of operations inherently increases with the system size $N$ due to larger quorum requirements, the system remains responsive. We observed that the overhead is largely dominated by initialization costs in smaller workloads, but the system scales efficiently for larger batches of operations. The experimental results validate that our implementation correctly balances the trade-off between strict consistency and the latency costs associated with distributed consensus.

\newpage
\begin{thebibliography}{9}

\bibitem{abd}
H. Attiya, A. Bar-Noy, and D. Dolev.
``Sharing memory robustly in message passing systems.''
\textit{Journal of the ACM}, 42(2):124--142, Jan. 1995.

\bibitem{course}
P. Kuznetsov.
``Storage in message passing systems.''
\textit{CSC\_4SL05\_TP/SLR206 Lecture Slides}, Télécom Paris, 2025.

\end{thebibliography}